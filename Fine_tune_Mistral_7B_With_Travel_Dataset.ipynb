{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 5111,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3899,
          "modelId": 1902
        }
      ],
      "dockerImageVersionId": 30588,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Fine-tune Mistral 7B With Travel Dataset",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oabai/finetune/blob/main/Fine_tune_Mistral_7B_With_Travel_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'mistral/pytorch/7b-v0.1-hf/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3899%2F5111%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240925%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240925T214314Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9a9951846af40302eab3d428bd420c3495388570b4888fdc5789d74ada6fc672ec494b9c7155f803ce237881902d871a2313b40edea73bf322e50fabc3c0501a67fe5f36e45ae3630d59938a740832636bdd87521b061655b10d7cbaf08baae1982c8e3ceda268cd06f8e2bb9ded1b9f0fdee2395363b808c6a1905c8449f337dff7313e538f61bdae05c39474f8d1643d1be8cd1ce4f9cbcdf9f62128eb69153d57b0f997173f061d4b5288254ce9142969ccb4280843323fe74a20ddfa18a8dd7770377c5412e325ad8aa64078f283bc38d434b1dcdbb841dd15fa9cd1c40dd2847b8732d037fe9cd8cce75068530de68c5a318675332f3ce3195212fd0780'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "aDoMh1T2I3p2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRAUKFG2I7K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Rpoq0hVI7dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U transformers\n",
        "%pip install -U peft\n",
        "%pip install -U accelerate\n",
        "%pip install -U trl\n",
        "%pip install -U datasets\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "d-qNTndQI3p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, wandb\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging"
      ],
      "metadata": {
        "trusted": true,
        "id": "9bFaTHL_I3p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.cache/huggingface/token\n",
        "\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
        "secret_wandb = user_secrets.get_secret(\"wandb\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "bcdpZIf7I3p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $secret_hf"
      ],
      "metadata": {
        "trusted": true,
        "id": "oLe8RfesI3p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monitering the LLM\n",
        "wandb.login(key = secret_wandb)\n",
        "run = wandb.init(\n",
        "    project='Fine tuning mistral 7B with moroccan Darija',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "vYQaNDTqI3p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define out params\n",
        "# This will error within kaggle using the base model,\n",
        "# use base model directly from HF for production i.e. mistralai/Mistral-7B-v0.1\n",
        "#base_model = \"mistralai/Mistral-7B-v0.1\"\n",
        "base_model = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
        "dataset_name = \"Digicactus/moroccantravel\"\n",
        "new_model = \"digicactus_7b_darija_moroccan\"\n",
        "padding_side = \"right\""
      ],
      "metadata": {
        "trusted": true,
        "id": "0QzwwilII3p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing a sample of our dataset\n",
        "train_dataset = load_dataset(dataset_name, split=\"train[0:300]\")\n",
        "eval_dataset = load_dataset(dataset_name, split=\"train[300:320]\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "y3IX3jk2I3p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.padding_side = padding_side\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.add_bos_token, tokenizer.add_eos_token"
      ],
      "metadata": {
        "trusted": true,
        "id": "KVbDKNWwI3p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to format the prompt\n",
        "def generate_prompt(sample):\n",
        "    full_prompt =f\"\"\"<s>[INST]{sample['input']}\n",
        " [/INST] {sample['response']}\n",
        "</s>\"\"\"\n",
        "    return {\"text\": full_prompt}"
      ],
      "metadata": {
        "trusted": true,
        "id": "JE4WpuyKI3p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_train_dataset = train_dataset.map(generate_prompt, remove_columns=list(train_dataset.features))\n",
        "generated_val_dataset = eval_dataset.map(generate_prompt, remove_columns=list(train_dataset.features))"
      ],
      "metadata": {
        "trusted": true,
        "id": "CJZZIWScI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the data to make sure all looks well\n",
        "generated_train_dataset[200]"
      ],
      "metadata": {
        "trusted": true,
        "id": "XB5aAuQQI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "id": "vsM_xLuDI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model (Mistral 7B)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        #load_in_4bit=True,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        ")\n",
        "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_jvsPbgBI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding the adapters in the layers\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1, # Coventional\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DQI5_YhpI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamter\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"wandb\",\n",
        "    evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "    eval_steps=25,               # Evaluate and save checkpoints every x steps\n",
        "    do_eval=True,                # Perform evaluation at the end of training\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LJz-I5wwI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting sft parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=generated_train_dataset,\n",
        "    eval_dataset=generated_val_dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=None,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "kDCv2pt0I3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZrGxLBAkI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned lora model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "wandb.finish()\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ],
      "metadata": {
        "trusted": true,
        "id": "IlJp__J8I3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will error within kaggle using the base model,\n",
        "# use base model directly from HF for production i.e. mistralai/Mistral-7B-v0.1\n",
        "try:\n",
        "    trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
        "except:\n",
        "    print(\"An exception occurred\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "BVGWvBhAI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "prompt = \"\"\"\n",
        "Est-il facile de trouver des distributeurs automatiques de billets au Maroc?\n",
        "\"\"\"\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"oabai/digicactus_7b_darija_moroccan-merged\", max_new_tokens=25)\n",
        "\n",
        "#pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, eos_token_id=model.config.eos_token_id, max_new_tokens=25)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "generated = result[0]['generated_text']\n",
        "print(generated[generated.find('[/INST]')+8:])"
      ],
      "metadata": {
        "trusted": true,
        "id": "-FQP1cO_I3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer"
      ],
      "metadata": {
        "trusted": true,
        "id": "CC71at57I3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "basemodel = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "#model = PeftModel.from_pretrained(basemodel, new_model) if you pushed lora to HF\n",
        "model = PeftModel.from_pretrained(basemodel, './results/checkpoint-50')\n",
        "model = model.merge_and_unload() # Merge lora back to base model\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = padding_side"
      ],
      "metadata": {
        "trusted": true,
        "id": "siTkMIjTI3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    model.push_to_hub(new_model + \"-merged\", max_shard_size='2GB')\n",
        "    tokenizer.push_to_hub(new_model + \"-merged\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "iO__a958I3p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
        "!git lfs install\n",
        "# Clone your model from Huggingface\n",
        "!git clone https://huggingface.co/oabai/digicactus_7b_darija_moroccan-merged\n",
        "# Clone llama.cpp's repository. They provide code to convert models into gguf.\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-24T12:09:15.389087Z",
          "iopub.execute_input": "2024-09-24T12:09:15.389921Z",
          "iopub.status.idle": "2024-09-24T12:11:17.746111Z",
          "shell.execute_reply.started": "2024-09-24T12:09:15.389891Z",
          "shell.execute_reply": "2024-09-24T12:11:17.744973Z"
        },
        "trusted": true,
        "id": "OdXqgiRuI3p6",
        "outputId": "3430dd8f-119e-4edd-cb16-24e0b6555af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Git LFS initialized.\nCloning into 'digicactus_7b_darija_moroccan-merged'...\nremote: Enumerating objects: 26, done.\u001b[K\nremote: Counting objects: 100% (23/23), done.\u001b[K\nremote: Compressing objects: 100% (23/23), done.\u001b[K\nremote: Total 26 (delta 3), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\nUnpacking objects: 100% (26/26), 467.90 KiB | 6.16 MiB/s, done.\nerror: unable to write file model-00002-of-00008.safetensors\nerror: unable to write file model-00007-of-00008.safetensors\nerror: unable to write file model-00005-of-00008.safetensors\nerror: unable to write file model-00003-of-00008.safetensors\nerror: unable to write file model-00004-of-00008.safetensors\nFiltering content: 100% (9/9), 4.33 GiB | 37.42 MiB/s, done.\nfatal: unable to checkout working tree\nwarning: Clone succeeded, but checkout failed.\nYou can inspect what was checked out with 'git status'\nand retry with 'git restore --source=HEAD :/'\n\nfatal: could not create work tree dir 'llama.cpp': No space left on device\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if colab\n",
        "!pip install -r /kaggle/working/llama.cpp/requirements.txt\n",
        "\n",
        "#if local then cd to cloned repo and perform following line\n",
        "# You can create venv as well\n",
        "#!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "wXdUlZ1kI3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for colab\n",
        "#         path to convert.py ⬇︎         path of model ⬇︎\n",
        "!python /kaggle/working/llama.cpp/convert.py /kaggle/working/digicactus_7b_darija_moroccan-merged  \\\n",
        "  --outfile finetuned-2.gguf \\ # gguf model name that you want to assign\n",
        "  --outtype q8_0 #quantize in 8-bit\n"
      ],
      "metadata": {
        "id": "ddOv0R0dI3p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That it! Find me at, or explore ML and SWE topics at [LivingTheCode.Life](https://livingthecode.life/)"
      ],
      "metadata": {
        "id": "NMt9Pc-iI3p7"
      }
    }
  ]
}